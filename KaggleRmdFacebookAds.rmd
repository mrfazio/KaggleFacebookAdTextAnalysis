---
title: "RmdFacebooksAds"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# A Gentle Introduction of Language Processing in R

The aim of this notebook is to show some techniques of language processing in R. We will explore the facebook ads purposed in this notebook and try to understand and get insights about the nature of the advertisements. We will begin by importing the used libraries and the dataset.



Remover variáveis
```{r}
rm(list=ls()) 
```


Pacotes necessários para o trabalho
```{r}
packages_list <- c('tidyverse',
'DT',
'tm',
'spacyr',
'SnowballC',
'RDSTK',
'wordcloud',
'stringi',
'gridExtra',
'RColorBrewer',
'textstem',
'rvest','stringr','igraph','gtools')

```

Executar somente na primeira vez que for executar
```{r}
new_packages <- packages_list[!(packages_list %in% installed.packages()[, "Package"])]

if (length(new_packages)){
  install.packages(new_packages, dependencies = TRUE)
}
```

Importação dos pacotes
```{r}
for (package in packages_list){
  library(package, character.only = TRUE)
}
```
Instalação do spacyr
```{r}
library("spacyr")
spacy_install()
```



```{r}
spacy_initialize(save_profile = TRUE)
```

Importação do dataset
```{r}
df_ads <- read_csv('fbpac-ads-en-US.csv')
```


Visualização das 10 primeiras linhas para verificar se o arquivo foi importado corretamente.
```{r}
head(df_ads , n = 10)
#print(colnames(df_ads))
```

Verificando as colunas do dataset
```{r}
print(colnames(df_ads))
```

We are ready to start and we will present each section as a simple sequence of steps. Let's go straightfoward and start!

# Basic EDA

## Checking Columns

As we can see, we have many columns. Let's understand each one of them and check if whether or not they are useful for our analysis:

* **ID**: Post id number on facebook
> Esta informação é a chave primária da base de dados. No nosso contexto, esta informação não tem nenhum significado para a análise sobre a publicidade. Podemos, então, removê-la.

* **html**: HTML of the ad as collected by the Political Ad Collector
> Não estamos interessados nas tags HTML, mas no conteúdo armazenado dentro das tags. Portanto, também iremos eliminar essa coluna.

* **political / not political**: Number of Political Ad Collector users who have voted that the ad is / isn't political
> Esses campos informam o número de usuários que votaram como a propaganda sendo política ou não. Esta informação nos parece interessante. Vamos mantê-la e explorá-la mais a frente.

* **title and message**
> Estas serão as principais colunas que iremos explorar por meio de técnicas de mineração de textos.

* 
* **thumbnail**: Link for a thumbnail of the profile image (of the advertiser)
> Não iremos explorar thumbnail, portanto podemos descartá-la.

* **created_at**: Date ad was first collected by the Political Ad Collector
> It's aways interesting to have a time dimension in our analysis. It allows us to, for instance, study the evolution of the ads as the time passes.

* **updated_at**: The most recent time it got an impression OR the most recent time it was voted on
> The same as the last column: we will keep this column.

* **lang**: Language
> It's aways english for the case of this dataset. So, there is no reason to keep this column.

## Created At / Updated At Analysis

Before filtering the columns, I just would like to check if the "created at" and "updated at" columns are equal (or if they are equal most of times):

Como existem 162.324 observações, iremos verificar o período de análise
```{r}
min(df_ads$created_at)
max(df_ads$created_at)
```

Definindo a probalidade da função
```{r}
hist(df_ads$political_probability)
```

Definindo as colunas que serão utilizadas do dataset
Também iremos utilzar uma janela de tempo com os dados a partir de jan/2019 e altamente políticos
```{r}
used_columns <- c('id','title', 'message')
data_set <- df_ads %>% 
  filter(created_at >= as.Date("2019-05-01")) %>% 
  filter(political_probability >= 0.9 ) %>% 
  select(one_of(used_columns))
```

Removendo a variável
```{r}
rm(df_ads)
```


# __Stemming__: Cleaning the Texts / Extracting Stems

We can clean the texts using the tm library:

```{r}
removeHTML <- function(x) (return(gsub('<.*?>' , '', x)))
removeURL <- function(x) (return(gsub('http[[:alnum:]]*', '', x)))
removeAccents <- function(x) (return(stri_trans_general(str = x,id = 'Latin-ASCII')))
removeNonAlphanum <- function(x) (return(gsub('[^[:alnum:] ]', '', x)))

cleaned <- paste(as.character(data_set$title), as.character(data_set$message), sep=" ")
cleaned <- tolower(cleaned)
cleaned <- removeHTML(cleaned)
cleaned <- removeURL(cleaned)
cleaned <- removeAccents(cleaned)
cleaned <- removeNonAlphanum(cleaned)

# remove any numbers from the titles
cleaned <- removeNumbers(cleaned)
# remove English stopwords
cleaned <- removeWords(cleaned, c(stopwords(kind = 'SMART'),'time','back','today','day','dont'))
# remove punctuation
cleaned <- removePunctuation(cleaned)
# remove spaces at the beginning and end of each title
cleaned <- str_trim(cleaned)
# convert vector of titles to a corpus


cleaned_corpus <- Corpus(VectorSource(cleaned))
# steam each word in each title
cleaned_corpus <- tm_map(cleaned_corpus, lemmatize_words)

doc_object <- TermDocumentMatrix(cleaned_corpus)
doc_matrix <- as.matrix(doc_object)  

```

The corpus function defines an object that can be manipulated with mapping functions. Those mapping functions will:

* Remove HTML tags
* Remove URL links
* Remove a set of stop words
* Strip white spaces of the strings
* Remove numbers and
* Remove punctuations

Also, the stemming operation will be applied: it removes the suffixes and prefixes of each word and then we will count words like "love" and "lovely" as a single word.  Finally, we will inspect the resulting object, that will be presented in the form of a matrix where the columns represent the posts and the rows represent the words.


We will start by checking the frequency of each word and showing the resulting table in a wordcloud scheme but, in order to do that, we need to group the words by the frequencies in the resulting dataframe:

```{r}

get_df_words_freq <- function(doc_matrix) {
  counts <- sort(rowSums(doc_matrix),decreasing=TRUE)
  # filter out any words that contain non-letters
  counts <- counts[grepl("^[a-z]+$", names(counts))]
  # create data frame from word frequency info
  frame_counts <- data.frame(word = names(counts), freq = counts)
  
  return(frame_counts)
  
}

df_words_freq <- get_df_words_freq(doc_matrix)

df_words_freq %>% datatable
```

WordCloud
```{r}
wordcloud(df_words_freq$word, freq = df_words_freq$freq, scale=c(2.5, 0.5), random.order=FALSE, max.words = 50, colors = brewer.pal(8, "Dark2"),rot.per=0,)
```



```{r}
spacy_initialize()
entities <- spacy_extract_entity(unlist(cleaned))
head(entities)
```

Colocando as entidades Pessoas e organizações no graphml
```{r}
# group entities by document
filtered_entities <- subset(entities, entities["ent_type"] == "ORG" | entities["ent_type"] == "PERSON" )

edges <- filtered_entities %>% dplyr::group_by(doc_id) %>% dplyr::summarize(entities = paste(text, collapse = ","))

# remove duplicated for the same document
edges <- lapply(str_split(edges$entities, ","), function(t){unique(unlist(t))})

# Auxiliary functions for creating adjancnt
get_adjacent_list <- function(edge_list) {
  adjacent_matrix <- combinations(length(edge_list), 2, edge_list,repeats.allowed=TRUE) 
  return(adjacent_matrix) 
}

adjacent_matrix <- edges %>% lapply(get_adjacent_list) %>% reduce(rbind)

df <- as_tibble(adjacent_matrix, colnames=c('source', 'target'))

weighted_edgelist <- df %>% dplyr::group_by(V1, V2) %>% dplyr::summarise(weight=n())

news_graph <- weighted_edgelist %>% graph_from_data_frame(directed=F)

write_graph(news_graph, 'news_graph.graphml', 'graphml')

```

Here we have an overall result, which makes sense: words like "Trump", "Million", "China" and "God" are among the most important ones. It's true that we have a general insight here but we can also check how the word clouds vary when we take political, neutral and non political posts. To do that, we will create $2$ documents:

* __Political__: We will gather all posts where most of people voted for "political" here.
* __Non Political__: We will gather all posts where most of people voted for "non political" here.

The posts where the number of votes for political and non political are similar will not be considered in this analysis.


$w_{i, j} = tf_{i, j} . log \left( \frac{N}{df_i} \right)$

Where $$tf_i, j$$ is the number of occurrences of the word "i" in the document "j", $$N$$ is the total number of documents and $$df_i$$ is the number of occurrences containing "i". To do that, we will merge political posts and non political posts in two "big strings" and apply our formula:

```{r}
political_msgs <- (df_ads %>% filter(political > not_political))$message
not_political_msgs <- (df_ads %>% filter(political < not_political))$message
```


```{r}
df_political_msgs_freq <- political_msgs %>% generate_corpus %>% get_df_words_freq
df_not_political_msgs_freq <- not_political_msgs %>% generate_corpus %>% get_df_words_freq
```


```{r}

wordcloud_political <- wordcloud(df_political_msgs_freq$Word, df_political_msgs_freq$Frequency, scale=c(2.5, 0.5), 
                                 max.words = 50, colors = brewer.pal(8, "Dark2"), random.color = TRUE)
```

```{r}

wordcloud_not_political <- wordcloud(df_not_political_msgs_freq$Word, df_not_political_msgs_freq$Frequency, 
                                     max.words = 50, scale=c(2.5, 0.5),
                                     colors = brewer.pal(8, "Dark2"), random.color = TRUE)
```

Those results really make sense: "God" is a common word in non political posts and words like "Trump", "Immigrants" and "Wage" are strongly present in political ads. So, we are ready to get the insights from those wordclouds and follow to the next part of our analysis.

Anyway, it's important to note that we have not exactly a set of "complete" words. In fact, we have used a "stemming" operation, which may be defined as:
> The process of reducing inflected (or sometimes derived) words to their word stem, base or root formâ€”generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root (Source: Wikipedia).

If we want to identify the "main words" that represent each kind of inflection, we are talking about "lemmatisation":
> Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. _Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research (Source: Wikipedia)_.

As already mentioned, the lemmatization demands a full comprehension of the context in which different words are located. So, we will now check an example of lemmatisation and to do that we will use the __SpacyR__ library.

# __Lemmatisation__: Improving the Insights by Checking Relationships

We can improve our analysis even more if we think that we have syntatical relationships among different words. So, we can use the __SpacyR__ library to get this analysis and improve the previous word clouds. We will be able to 

