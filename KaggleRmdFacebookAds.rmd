---
title: "RmdFacebooksAds"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# A Gentle Introduction of Language Processing in R

The aim of this notebook is to show some techniques of language processing in R. We will explore the facebook ads purposed in this notebook and try to understand and get insights about the nature of the advertisements. We will begin by importing the used libraries and the dataset.

Pacotes necessários para o trabalho
```{r}
rm(list=ls()) 

lista_de_pacotes <- c('tidyverse',
'DT',
'tm',
'spacyr',
'SnowballC',
'RDSTK',
'wordcloud',
'stringi',
'gridExtra',
'RColorBrewer')

novos_pacotes <- lista_de_pacotes[!(lista_de_pacotes %in% installed.packages()[, "Package"])]

if (length(novos_pacotes))
  install.packages(novos_pacotes, dependencies = TRUE)

for (package in lista_de_pacotes){
  library(package, character.only = TRUE)
}
```

Leitura do arquivo
```{r}
df_ads <- read_csv('fbpac-ads-en-US.csv')

```


Visualização das 10 primeiras linhas para verificar se o arquivo foi importado corretamente.
```{r}
head(df_ads , n = 10)

#print(colnames(df_ads))
```

Verificando as colunas do dataset
```{r}
print(colnames(df_ads))
```

We are ready to start and we will present each section as a simple sequence of steps. Let's go straightfoward and start!

# Basic EDA

## Checking Columns

As we can see, we have many columns. Let's understand each one of them and check if whether or not they are useful for our analysis:

* **ID**: Post id number on facebook
> Esta informação é a chave primária da base de dados. No nosso contexto, esta informação não tem nenhum significado para a análise sobre a publicidade. Podemos, então, removê-la.

* **html**: HTML of the ad as collected by the Political Ad Collector
> Não estamos interessados nas tags HTML, mas no conteúdo armazenado dentro das tags. Portanto, também iremos eliminar essa coluna.

* **political / not political**: Number of Political Ad Collector users who have voted that the ad is / isn't political
> Esses campos informam o número de usuários que votaram como a propaganda sendo política ou não. Esta informação nos parece interessante. Vamos mantê-la e explorá-la mais a frente.

* **title and message**
> Estas serão as principais colunas que iremos explorar por meio de técnicas de mineração de textos.

* **thumbnail**: Link for a thumbnail of the profile image (of the advertiser)
> Não iremos explorar thumbnail, portanto podemos descartá-la.

* **created_at**: Date ad was first collected by the Political Ad Collector
> It's aways interesting to have a time dimension in our analysis. It allows us to, for instance, study the evolution of the ads as the time passes.

* **updated_at**: The most recent time it got an impression OR the most recent time it was voted on
> The same as the last column: we will keep this column.

* **lang**: Language
> It's aways english for the case of this dataset. So, there is no reason to keep this column.

## Created At / Updated At Analysis

Before filtering the columns, I just would like to check if the "created at" and "updated at" columns are equal (or if they are equal most of times):

```{r}
ggplot(df_ads %>% mutate(`Was Updated` = (created_at == updated_at)), 
       aes(x = `Was Updated`, fill = `Was Updated`)) + 
  geom_histogram(stat = 'count',color = 'black') +
  ggtitle('Was the Ad Updated at Least Once?') +
  ylab('Number of Times') + theme(text = element_text(size = 13))
```

Ok. We can see that we get updated ads frequently. So, we will keep both columns (created_at and updated_at). We can also check the time that it takes to update a post and see the histogram of that difference:

```{r}
ggplot(df_ads %>% mutate(`Update Time` = as.double(difftime(updated_at, created_at)) / (60 * 60 * 24)), 
       aes(x = `Update Time`)) + 
  geom_histogram(color = 'black', fill = 'lightblue') +
  ggtitle('Updating Time Histogram (Days)') +
  ylab('Count') + theme(text = element_text(size = 13)) +
  scale_y_sqrt() + scale_x_sqrt()
```

We can also ask ourselves: are those distributions similar for political and non political posts?

```{r}
ggplot(df_ads %>% 
         mutate(`Update Time` = as.double(difftime(updated_at, created_at)) / (60 * 60 * 24)) %>% 
         mutate(Political = political > not_political), 
       aes(x = `Update Time`)) + 
  geom_histogram(color = 'black', aes(fill = Political)) +
  ggtitle('Updating Time Histogram (Days)') +
  ylab('Count') + theme(text = element_text(size = 13)) +
  scale_x_sqrt() + scale_y_sqrt()
```

Here we can notice that if we have no update, it's probable that we are dealing with a non politcal post. If we have at least one update, it becames more probable that we are dealing with a political post! It confirms what we saw in the last plot: people tend to vote in political posts!

## Filtering the Columns

Let's filter our dataframe and check the structure of the new table:

```{r}
used_columns <- c('title', 'message', 'political', 'not_political', 'created_at', 'updated_at')
df_ads <- df_ads %>% select(one_of(used_columns))
df_ads %>% head %>% datatable
```

## Votes Confidence Intervals

We can also check the counting of the political / not political votes:

```{r}
df_political_distribution <- data.frame(
  Vote = c(df_ads$political, df_ads$not_political),
  Type = c(rep('Political', length(df_ads$political)), rep('Not Political', length(df_ads$not_political)))
)

ggplot(df_political_distribution, aes(x = Vote, color = Type)) +
  geom_density2d(stat = 'count') + ylab('Count') +
  theme(text = element_text(size = 13)) + scale_x_sqrt() + scale_y_sqrt() +
  ggtitle('Political / Not Political Count')
```

If we have more votes we tend to have a bigger chance to deal with political posts. It seems to happen because of a bias in the feelings of the users as we already seen in the previous section: when a user finds a political post, he tends to vote with a higher probability thanks to a sentiment of displeasure and engagement.

Let's create a new feature: the % of people that thinks that the ad has a political nature. This indicator may be associated with a standard deviation of a proportions test, where the standard deviation is given by:

$$ \sigma^2 = \frac{p.(1 - p)}{N} $$

Where "N" is the total number of voters of the ad in analysis and "p" is the proportio of people that thinks that a post is political:

$$ p = \frac{V_{Political}}{N} $$

It's possible to estimate the standard deviation because the proportion may be seen as the average of variables that follow a Bernouilli's distribution and the average of these variables converge to a normal distribution as "N" tends to infinity. With this concept, we are able to estimate the standard deviation and define confidence intervals as well as p-value analysis!

```{r}
df_ads <- df_ads %>%
  mutate(political_prob = 
           if_else(political + not_political < 30, NaN, 
                                  political / (political + not_political))) %>% 
  mutate(political_prob_std = 
           sqrt(political_prob * (1 - political_prob) / (political + not_political) ))

datatable(df_ads %>% select(one_of(c('title', 'political', 'not_political', 'political_prob', 'political_prob_std'))))
```

Here we are following a rule of thumb that says that we need at least $30$ samples to consider that we will have a good statistical test and a good convergence to the normal distribution. What are we going to do with the other samples? Well, can't get many conclusions. Anyway, we already have some nice results here: we  have a set of confidence intervals for some text messages:

```{r}
df_ads %>% 
  filter(!is.na(political_prob)) %>% 
  select(one_of(c('message', 'political_prob', 'political_prob_std'))) %>% 
  datatable
```

Let's show an example:

```{r, fig.width = 11, fig.height = 5, echo = FALSE}
df_example <- data.frame(
  Message = c('The Global Gag Rule puts lives at risk 
    and limits access to modern 
    contraceptives around the world. 
    Take action with Population Connection now: 
    Say NO to the Global Gag Rule!',
    'As Governor of New York, my top priority 
    is the safety of every New Yorker',
    'Innovation in medicine is bringing potential 
    cures to some of the most complex diseases 
    we face, like cancer, rheumatoid arthritis, 
    and Crohnâ€™s disease.'),
  `Prob` = c(0.707, 0.958, 0.68),
  Std = c(0.071, 0.0288, 0.0659)
)

ggplot(df_example, aes(x = Message, y = Prob, ymin = Prob - 3 * Std, ymax = Prob + 3 * Std)) + 
  geom_pointrange(size = 1.2, shape = 21, color = 'black', aes(fill = Message)) +
  theme(text = element_text(size = 14), 
        axis.text.x = element_text(angle = 0, hjust = 0.5),
        legend.position = 'none') + ylab('Political Probability') + 
  ggtitle('Proportion Confidence Intervals')
```

Well, I got $3$ examples here manualy but if you check the messages you will see that it's necessary to clean the text and prepare it to be processed and analysed.

# __Stemming__: Cleaning the Texts / Extracting Stems

We can clean the texts using the tm library:

```{r}
removeHTML <- function(x) (return(gsub('</?[^>]+>' , '', x)))
removeURL <- function(x) (return(gsub('http[[:alnum:]]*', '', x)))
removeAccents <- function(x) (return(stri_trans_general(str = x,id = 'Latin-ASCII')))
removeNonAlphanum <- function(x) (return(gsub('[^[:alnum:] ]', '', x)))

generate_corpus <- function(vector_in) {
  
  VectorSource(vector_in) %>% Corpus %>% 
    
  tm_map(content_transformer(removeHTML)) %>% 
  tm_map(content_transformer(removeURL)) %>% 
  tm_map(content_transformer(removeAccents)) %>% 
  tm_map(content_transformer(removeNonAlphanum)) %>%
    
  tm_map(removeWords, stopwords(kind = 'en')) %>% 
  tm_map(stripWhitespace) %>% 
  tm_map(removeNumbers) %>% 
  tm_map(removePunctuation) %>% 
  tm_map(stemDocument) %>%
    
  return()
  
}

vector_msg_raw <- df_ads$message
vector_msg_clean <- generate_corpus(vector_msg_raw)
  
```

The corpus function defines an object that can be manipulated with mapping functions. Those mapping functions will:

* Remove HTML tags
* Remove URL links
* Remove a set of stop words
* Strip white spaces of the strings
* Remove numbers and
* Remove punctuations

Also, the stemming operation will be applied: it removes the suffixes and prefixes of each word and then we will count words like "love" and "lovely" as a single word.  Finally, we will inspect the resulting object, that will be presented in the form of a matrix where the columns represent the posts and the rows represent the words.

```{r}
termDocMat <- TermDocumentMatrix(vector_msg_clean)
inspect(termDocMat)
```

We will start by checking the frequency of each word and showing the resulting table in a wordcloud scheme but, in order to do that, we need to group the words by the frequencies in the resulting dataframe:

```{r}

get_df_words_freq <- function(vector_msg_clean_in) {
  
  termDocMat_in <- TermDocumentMatrix(vector_msg_clean_in)
  matFreq <- as.matrix(findMostFreqTerms(termDocMat_in, n = 100))
  flat_matFreq <- flatten(matFreq)
  df_words_freq <- data.frame(Word = names(flat_matFreq), Frequency = unlist(flat_matFreq))
  df_words_freq <- unique(df_words_freq) %>% arrange(desc(Frequency))
  
  return(df_words_freq)
  
}

df_words_freq <- vector_msg_clean %>% get_df_words_freq
df_words_freq %>% datatable
```

```{r}
wordcloud(df_words_freq$Word, freq = df_words_freq$Frequency, scale=c(2.5, 0.5),
          random.color = TRUE, max.words = 50, colors = brewer.pal(8, "Dark2"))
```

Here we have an overall result, which makes sense: words like "Trump", "Million", "China" and "God" are among the most important ones. It's true that we have a general insight here but we can also check how the word clouds vary when we take political, neutral and non political posts. To do that, we will create $2$ documents:

* __Political__: We will gather all posts where most of people voted for "political" here.
* __Non Political__: We will gather all posts where most of people voted for "non political" here.

The posts where the number of votes for political and non political are similar will not be considered in this analysis.


$w_{i, j} = tf_{i, j} . log \left( \frac{N}{df_i} \right)$

Where $$tf_i, j$$ is the number of occurrences of the word "i" in the document "j", $$N$$ is the total number of documents and $$df_i$$ is the number of occurrences containing "i". To do that, we will merge political posts and non political posts in two "big strings" and apply our formula:

```{r}
political_msgs <- (df_ads %>% filter(political > not_political))$message
not_political_msgs <- (df_ads %>% filter(political < not_political))$message
```


```{r}
df_political_msgs_freq <- political_msgs %>% generate_corpus %>% get_df_words_freq
df_not_political_msgs_freq <- not_political_msgs %>% generate_corpus %>% get_df_words_freq
```


```{r}

wordcloud_political <- wordcloud(df_political_msgs_freq$Word, df_political_msgs_freq$Frequency, scale=c(2.5, 0.5), 
                                 max.words = 50, colors = brewer.pal(8, "Dark2"), random.color = TRUE)
```

```{r}

wordcloud_not_political <- wordcloud(df_not_political_msgs_freq$Word, df_not_political_msgs_freq$Frequency, 
                                     max.words = 50, scale=c(2.5, 0.5),
                                     colors = brewer.pal(8, "Dark2"), random.color = TRUE)
```

Those results really make sense: "God" is a common word in non political posts and words like "Trump", "Immigrants" and "Wage" are strongly present in political ads. So, we are ready to get the insights from those wordclouds and follow to the next part of our analysis.

Anyway, it's important to note that we have not exactly a set of "complete" words. In fact, we have used a "stemming" operation, which may be defined as:
> The process of reducing inflected (or sometimes derived) words to their word stem, base or root formâ€”generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root (Source: Wikipedia).

If we want to identify the "main words" that represent each kind of inflection, we are talking about "lemmatisation":
> Lemmatisation (or lemmatization) in linguistics is the process of grouping together the inflected forms of a word so they can be analysed as a single item, identified by the word's lemma, or dictionary form. In computational linguistics, lemmatisation is the algorithmic process of determining the lemma of a word based on its intended meaning. _Unlike stemming, lemmatisation depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document. As a result, developing efficient lemmatisation algorithms is an open area of research (Source: Wikipedia)_.

As already mentioned, the lemmatization demands a full comprehension of the context in which different words are located. So, we will now check an example of lemmatisation and to do that we will use the __SpacyR__ library.

# __Lemmatisation__: Improving the Insights by Checking Relationships

We can improve our analysis even more if we think that we have syntatical relationships among different words. So, we can use the __SpacyR__ library to get this analysis and improve the previous word clouds. We will be able to 

